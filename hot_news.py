#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ å¿…è¦çš„å¯¼å…¥
import os
import hashlib
import pickle
from pathlib import Path
from datetime import datetime, timedelta
import requests
import json
import time
import argparse
from datetime import datetime, timedelta
import logging
import os
import sys
import feedparser  # æ–°å¢ï¼šç”¨äºè§£æRSS/Atom feed
import asyncio
from concurrent.futures import ThreadPoolExecutor
from langchain_community.document_loaders import WebBaseLoader
from langchain.chains import LLMChain
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
import re

from bs4 import BeautifulSoup
from dateutil import parser as date_parser

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ç§‘æŠ€ç›¸å…³çš„ä¿¡æ¯æºåˆ—è¡¨
TECH_SOURCES = [
    # "bilibili",     # å«å¤§é‡ç§‘æŠ€åŒºUPä¸»ï¼ˆè¯„æµ‹/æ•™ç¨‹/æå®¢ï¼‰
    "zhihu",        # ç§‘æŠ€ç±»é—®ç­”å’Œä¸“æ æ–‡ç« 
    "sspai",        # ä¸“æ³¨æ•ˆç‡å·¥å…·å’Œç§‘æŠ€åº”ç”¨
    "ithome",       # ITç§‘æŠ€æ–°é—»é—¨æˆ·
    "36kr",         # ç§‘æŠ€åˆ›æ–°åˆ›ä¸šèµ„è®¯å¹³å°
    "juejin",       # å¼€å‘è€…æŠ€æœ¯ç¤¾åŒº
    "csdn",         # ä¸“ä¸šæŠ€æœ¯åšå®¢å¹³å°
    "51cto",        # ITæŠ€æœ¯è¿ç»´ç¤¾åŒº  
    "huxiu",        # ç§‘æŠ€å•†ä¸šåª’ä½“
    "ifanr",        # èšç„¦æ™ºèƒ½ç¡¬ä»¶çš„ç§‘æŠ€åª’ä½“
    "coolapk",      # å®‰å“åº”ç”¨å’Œç§‘æŠ€äº§å“è®¨è®º
    "v2ex",         # åˆ›æ„å·¥ä½œè€…æŠ€æœ¯ç¤¾åŒº
    "hostloc",      # æœåŠ¡å™¨å’Œç½‘ç»œæŠ€æœ¯äº¤æµ
    "hupu",         # è™æ‰‘æ•°ç åŒºï¼ˆæ‰‹æœº/ç”µè„‘è®¨è®ºï¼‰
    "guokr",        # æ³›ç§‘å­¦ç§‘æ™®å¹³å°
    "hellogithub",  # GitHubå¼€æºé¡¹ç›®æ¨è
    "nodeseek",     # æœåŠ¡å™¨å’Œç½‘ç»œæŠ€æœ¯è®ºå›
    "52pojie",      # è½¯ä»¶é€†å‘æŠ€æœ¯ç¤¾åŒº
    "ithome-xijiayi",# å…è´¹è½¯ä»¶/æ¸¸æˆèµ„è®¯
    "zhihu-daily",  # å«ç§‘æŠ€ç±»æ·±åº¦æŠ¥é“
    "tieba",        # ç™¾åº¦è´´å§ï¼ˆæ‰‹æœº/ç”µè„‘ç›¸å…³è´´å§ï¼‰
]

# æ‰€æœ‰å¯ç”¨çš„ä¿¡æ¯æº
ALL_SOURCES = [
    "bilibili",   # å“”å“©å“”å“©
    "weibo",      # å¾®åš
    "zhihu",      # çŸ¥ä¹
    "baidu",      # ç™¾åº¦
    "douyin",     # æŠ–éŸ³
    "kuaishou",   # å¿«æ‰‹
    "tieba",      # ç™¾åº¦è´´å§
    "sspai",      # å°‘æ•°æ´¾
    "ithome",     # ITä¹‹å®¶
    "toutiao",    # ä»Šæ—¥å¤´æ¡
    "36kr",       # 36æ°ª
    "juejin",     # æ˜é‡‘
    "csdn",       # CSDN
    "51cto",      # 51CTO
    "huxiu",      # è™å—…
    "ifanr",      # çˆ±èŒƒå„¿
    "coolapk",    # é…·å®‰
    "hupu",       # è™æ‰‘
    "v2ex",       # V2EX
    "hostloc",    # å…¨çƒä¸»æœºäº¤æµ
    "sina-news",  # æ–°æµªæ–°é—»
    "netease-news", # ç½‘æ˜“æ–°é—»
    "qq-news",    # è…¾è®¯æ–°é—»
    "thepaper",   # æ¾æ¹ƒæ–°é—»
    "jianshu",    # ç®€ä¹¦
    "guokr",      # æœå£³
    "acfun",      # AcFun
    "douban-movie", # è±†ç“£ç”µå½±
    "douban-group", # è±†ç“£è®¨è®ºå°ç»„
    "zhihu-daily", # çŸ¥ä¹æ—¥æŠ¥
    "ithome-xijiayi", # ITä¹‹å®¶ã€Œå–œåŠ ä¸€ã€
    "ngabbs",     # NGA
    "hellogithub", # HelloGitHub
    "nodeseek",   # NodeSeek
    "miyoushe",   # ç±³æ¸¸ç¤¾
    "genshin",    # åŸç¥
    "honkai",     # å´©å3
    "starrail",   # å´©åï¼šæ˜Ÿç©¹é“é“
    "weread",     # å¾®ä¿¡è¯»ä¹¦
    "lol",        # è‹±é›„è”ç›Ÿ
    "52pojie",    # å¾çˆ±ç ´è§£
]

def save_hotspots_to_jsonl(hotspots, directory="data"):
    """
    å°†çƒ­ç‚¹æ•°æ®ä¿å­˜ä¸ºJSONLæ ¼å¼ï¼ŒæŒ‰æ—¥æœŸç»„ç»‡ï¼Œä½¿ç”¨ç›¸å¯¹è·¯å¾„
    """
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(directory, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶åï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
        today = datetime.now().strftime("%Y-%m-%d")
        timestamp = datetime.now().strftime("%H-%M-%S")
        filename = os.path.join(directory, f"hotspots_{today}_{timestamp}.jsonl")
        
        # å†™å…¥JSONLæ–‡ä»¶
        with open(filename, 'w', encoding='utf-8') as f:
            for item in hotspots:
                # æ·»åŠ æ—¶é—´æˆ³
                item_with_timestamp = item.copy()
                item_with_timestamp['saved_at'] = datetime.now().isoformat()
                f.write(json.dumps(item_with_timestamp, ensure_ascii=False) + '\n')
        
        logger.info(f"å·²å°† {len(hotspots)} æ¡çƒ­ç‚¹æ•°æ®ä¿å­˜è‡³ {filename}")
        return filename
    except Exception as e:
        logger.error(f"ä¿å­˜çƒ­ç‚¹æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return None

# æ·»åŠ ç¼“å­˜ç›¸å…³å‡½æ•°
def get_content_hash(content):
    """
    è®¡ç®—å†…å®¹çš„å“ˆå¸Œå€¼ï¼Œç”¨äºç¼“å­˜æ ‡è¯†
    """
    if not content:
        return None
    return hashlib.md5(content.encode('utf-8')).hexdigest()

def load_summary_cache(cache_dir="cache/summary"):
    """
    åŠ è½½æ‘˜è¦ç¼“å­˜
    """
    try:
        cache_path = Path(cache_dir) / "summary_cache.pkl"
        if not cache_path.exists():
            return {}
        
        with open(cache_path, 'rb') as f:
            cache = pickle.load(f)
            logger.info(f"å·²åŠ è½½æ‘˜è¦ç¼“å­˜ï¼ŒåŒ…å« {len(cache)} æ¡è®°å½•")
            return cache
    except Exception as e:
        logger.warning(f"åŠ è½½æ‘˜è¦ç¼“å­˜å¤±è´¥: {str(e)}")
        return {}

def save_summary_cache(cache, cache_dir="cache/summary"):
    """
    ä¿å­˜æ‘˜è¦ç¼“å­˜
    """
    try:
        cache_path = Path(cache_dir)
        cache_path.mkdir(parents=True, exist_ok=True)
        
        with open(cache_path / "summary_cache.pkl", 'wb') as f:
            pickle.dump(cache, f)
            logger.info(f"å·²ä¿å­˜æ‘˜è¦ç¼“å­˜ï¼Œå…± {len(cache)} æ¡è®°å½•")
    except Exception as e:
        logger.warning(f"ä¿å­˜æ‘˜è¦ç¼“å­˜å¤±è´¥: {str(e)}")


def check_base_url(base_url):
    """
    æ£€æŸ¥ BASE_URL æ˜¯å¦å¯è®¿é—®
    """
    try:
        response = requests.get(f"{base_url}/bilibili?limit=5", timeout=10)
        response.raise_for_status()
        data = response.json()
        if data.get("code") == 200:
            logger.info(f"BASE_URL æ£€æŸ¥é€šè¿‡: {base_url}")
            return True
        else:
            logger.error(f"BASE_URL è¿”å›é”™è¯¯: {data}")
            return False
    except Exception as e:
        logger.error(f"BASE_URL æ£€æŸ¥å¤±è´¥: {str(e)}")
        return False

def fetch_hotspot(source, base_url):
    """
    ä»æŒ‡å®šæºè·å–çƒ­ç‚¹æ•°æ®
    """
    try:
        url = f"{base_url}/{source}?limit=10"
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        data = response.json()
        
        if data.get("code") == 200:
            return data.get("data", [])
        else:
            logger.error(f"è·å– {source} æ•°æ®å¤±è´¥: {data.get('message', 'æœªçŸ¥é”™è¯¯')}")
            return []
    except Exception as e:
        logger.error(f"è·å– {source} æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return []

def collect_all_hotspots(sources, base_url):
    """
    æ”¶é›†æ‰€æœ‰æŒ‡å®šæºçš„çƒ­ç‚¹æ•°æ®
    """
    all_hotspots = []
    
    for source in sources:
        logger.info(f"æ­£åœ¨è·å– {source} çš„çƒ­ç‚¹æ•°æ®...")
        hotspots = fetch_hotspot(source, base_url)
        
        for item in hotspots:
            # ç¡®ä¿æ¯ä¸ªçƒ­ç‚¹éƒ½æœ‰æ ‡é¢˜å’Œé“¾æ¥
            if "title" in item and "url" in item:
                # æ„å»ºçƒ­ç‚¹æ•°æ®ï¼Œä¿ç•™descå­—æ®µ
                hotspot_data = {
                    "title": item["title"],
                    "url": item["url"],
                    "source": source,
                    "hot": item.get("hot", ""),
                    "time": item.get("time", ""),
                    "timestamp": item.get("timestamp", ""),
                }
                
                # å¦‚æœæœ‰æ‘˜è¦ï¼Œä¿ç•™æ‘˜è¦
                if "desc" in item and item["desc"]:
                    hotspot_data["desc"] = item["desc"]
                    
                all_hotspots.append(hotspot_data)
    
    logger.info(f"å…±æ”¶é›†åˆ° {len(all_hotspots)} æ¡çƒ­ç‚¹æ•°æ®")
    return all_hotspots

# æ·»åŠ æºåç§°æ˜ å°„å­—å…¸
SOURCE_NAME_MAP = {
    "bilibili": "å“”å“©å“”å“©",
    "weibo": "å¾®åš",
    "zhihu": "çŸ¥ä¹",
    "baidu": "ç™¾åº¦",
    "douyin": "æŠ–éŸ³",
    "kuaishou": "å¿«æ‰‹",
    "tieba": "ç™¾åº¦è´´å§",
    "sspai": "å°‘æ•°æ´¾",
    "ithome": "ITä¹‹å®¶",
    "toutiao": "ä»Šæ—¥å¤´æ¡",
    "36kr": "36æ°ª",
    "juejin": "æ˜é‡‘",
    "csdn": "CSDN",
    "51cto": "51CTO",
    "huxiu": "è™å—…",
    "ifanr": "çˆ±èŒƒå„¿",
    "coolapk": "é…·å®‰",
    "hupu": "è™æ‰‘",
    "v2ex": "V2EX",
    "hostloc": "å…¨çƒä¸»æœºäº¤æµ",
    "sina-news": "æ–°æµªæ–°é—»",
    "netease-news": "ç½‘æ˜“æ–°é—»",
    "qq-news": "è…¾è®¯æ–°é—»",
    "thepaper": "æ¾æ¹ƒæ–°é—»",
    "jianshu": "ç®€ä¹¦",
    "guokr": "æœå£³",
    "acfun": "AcFun",
    "douban-movie": "è±†ç“£ç”µå½±",
    "douban-group": "è±†ç“£è®¨è®ºå°ç»„",
    "zhihu-daily": "çŸ¥ä¹æ—¥æŠ¥",
    "ithome-xijiayi": "ITä¹‹å®¶å–œåŠ ä¸€",
    "ngabbs": "NGA",
    "hellogithub": "HelloGitHub",
    "nodeseek": "NodeSeek",
    "miyoushe": "ç±³æ¸¸ç¤¾",
    "genshin": "åŸç¥",
    "honkai": "å´©å3",
    "starrail": "å´©åï¼šæ˜Ÿç©¹é“é“",
    "weread": "å¾®ä¿¡è¯»ä¹¦",
    "lol": "è‹±é›„è”ç›Ÿ",
    "52pojie": "å¾çˆ±ç ´è§£",
}
def format_title_for_display(title, source, max_length=30):
    """
    æ ¼å¼åŒ–æ ‡é¢˜ï¼Œç¡®ä¿é•¿åº¦ä¸€è‡´ï¼Œé€‚é…æ‰‹æœºå®½åº¦
    """
    # è®¡ç®—æ ‡é¢˜æœ€å¤§é•¿åº¦ï¼ˆè€ƒè™‘åˆ°åé¢è¦åŠ ä¸Šæ¥æºï¼‰
    source_part = f" - {source}"
    title_max_length = max_length - len(source_part)
    
    # å¦‚æœæ ‡é¢˜å¤ªé•¿ï¼Œæˆªæ–­å¹¶æ·»åŠ çœç•¥å·
    if len(title) > title_max_length:
        title = title[:title_max_length-1] + "â€¦"
    
    # è¿”å›æ ¼å¼åŒ–åçš„æ ‡é¢˜
    return f"{title}{source_part}"
# ä¿®æ”¹fetch_webpage_contentå‡½æ•°ï¼Œè¿”å›åŸå§‹HTMLå†…å®¹
def fetch_webpage_content(url, timeout=10, max_retries=3):
    """
    è·å–ç½‘é¡µå†…å®¹ï¼Œè¿”å›å¤„ç†åçš„æ–‡æœ¬å†…å®¹å’ŒåŸå§‹HTML
    """
    retry_count = 0
    while retry_count < max_retries:
        try:
            # è®¾ç½®æ›´å¤šé€‰é¡¹ä»¥æé«˜ç¨³å®šæ€§
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            }
            response = requests.get(url, headers=headers, timeout=timeout, verify=False)
            response.raise_for_status()
            
            # è·å–åŸå§‹HTMLå†…å®¹
            html_content = response.text
            
            # ä½¿ç”¨BeautifulSoupå¤„ç†HTML
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æå–æ–‡æœ¬å†…å®¹
            text_content = soup.get_text(separator=' ', strip=True)
            
            # é¢„å¤„ç†æ–‡æœ¬å†…å®¹
            processed_content = preprocess_webpage_content(text_content)
            
            logger.info(f"è·å–åˆ°ç½‘é¡µå†…å®¹: {url}, åŸå§‹HTMLé•¿åº¦: {len(html_content)}, å¤„ç†åæ–‡æœ¬é•¿åº¦: {len(processed_content)} å­—ç¬¦")
            
            return processed_content, html_content
        except Exception as e:
            retry_count += 1
            if retry_count < max_retries:
                logger.warning(f"è·å–ç½‘é¡µå†…å®¹å¤±è´¥: {url}, é”™è¯¯: {str(e)}ï¼Œ5ç§’åé‡è¯• ({retry_count}/{max_retries})...")
                time.sleep(5)
            else:
                logger.error(f"è·å–ç½‘é¡µå†…å®¹å¤±è´¥: {url}, é”™è¯¯: {str(e)}")
                return "", ""

def preprocess_webpage_content(content):
    """
    é¢„å¤„ç†ç½‘é¡µå†…å®¹ï¼Œå»é™¤æ— å…³å†…å®¹ï¼Œæå–æ ¸å¿ƒæ–‡æœ¬
    """
    if not content:
        return ""
    
    # 1. å»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
    content = ' '.join(content.split())
    
    # 2. å»é™¤å¸¸è§çš„ç½‘é¡µå™ªéŸ³
    noise_patterns = [
        r'ç‰ˆæƒæ‰€æœ‰.*?ä¿ç•™æ‰€æœ‰æƒåˆ©',
        r'Copyright.*?Reserved',
        r'å…è´£å£°æ˜.*?',
        r'éšç§æ”¿ç­–.*?',
        r'ç™»å½•.*?æ³¨å†Œ',
        r'å…³æ³¨æˆ‘ä»¬.*?',
        r'ç‚¹å‡»æŸ¥çœ‹.*?',
        r'ç›¸å…³é˜…è¯».*?',
        r'çŒœä½ å–œæ¬¢.*?',
        r'å¹¿å‘Š.*?',
        r'è¯„è®º.*?',
    ]
    
    import re
    for pattern in noise_patterns:
        content = re.sub(pattern, ' ', content, flags=re.IGNORECASE)
    
    # 3. å¦‚æœå†…å®¹å¤ªé•¿ï¼Œä¿ç•™å‰2000å­—ç¬¦ï¼ˆè€ƒè™‘åˆ°åç»­ä¼šæˆªæ–­ï¼‰
    if len(content) > 3000:
        # è®°å½•æˆªæ–­ä¿¡æ¯
        logger.info(f"å†…å®¹è¿‡é•¿ï¼Œä» {len(content)} å­—ç¬¦æˆªæ–­è‡³ 3000 å­—ç¬¦")
        
        # å°è¯•åœ¨å¥å­è¾¹ç•Œæˆªæ–­
        sentences = re.split(r'[.ã€‚!ï¼?ï¼Ÿ;ï¼›]', content[:3000])
        if len(sentences) > 1:
            # ä¿ç•™å®Œæ•´å¥å­
            content = '.'.join(sentences[:-1]) + '.'
        else:
            content = content[:3000]
    
    return content
# ä¿®æ”¹summarize_with_tencent_hunyuanå‡½æ•°ï¼Œæ·»åŠ ç¼“å­˜åŠŸèƒ½
def summarize_with_tencent_hunyuan(content, api_key, max_retries=3, use_cache=True):
    """
    ä½¿ç”¨è…¾è®¯æ··å…ƒturbo-Sæ¨¡å‹å¯¹å†…å®¹è¿›è¡Œæ¦‚è¿°æ€»ç»“
    è¿”å›JSONæ ¼å¼ï¼ŒåŒ…å«æ‘˜è¦å’Œç§‘æŠ€ç›¸å…³æ€§åˆ¤æ–­
    æ”¯æŒç¼“å­˜æœºåˆ¶ï¼Œé¿å…é‡å¤å¤„ç†ç›¸åŒå†…å®¹
    """
    if not content or len(content.strip()) < 50:
        logger.warning(f"å†…å®¹è¿‡çŸ­æˆ–ä¸ºç©ºï¼Œè·³è¿‡æ‘˜è¦ç”Ÿæˆ: {content[:50]}...")
        return {"summary": "", "is_tech": False}
    
    # è®¡ç®—å†…å®¹å“ˆå¸Œå€¼ç”¨äºç¼“å­˜
    content_hash = get_content_hash(content[:2000])  # åªå¯¹å‰2000å­—ç¬¦è®¡ç®—å“ˆå¸Œ
    
    # å¦‚æœå¯ç”¨ç¼“å­˜ï¼Œå°è¯•ä»ç¼“å­˜ä¸­è·å–ç»“æœ
    if use_cache and content_hash:
        # åŠ è½½ç¼“å­˜
        summary_cache = load_summary_cache()
        
        # æ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦æœ‰å¯¹åº”çš„ç»“æœ
        if content_hash in summary_cache:
            cached_result = summary_cache[content_hash]
            logger.info(f"ä»ç¼“å­˜ä¸­è·å–æ‘˜è¦: {cached_result['summary'][:30]}...")
            return cached_result
    
    retry_count = 0
    while retry_count < max_retries:
        try:
            # è®°å½•è¦å‘é€çš„å†…å®¹é•¿åº¦
            logger.info(f"å‘é€è‡³æ··å…ƒæ¨¡å‹çš„å†…å®¹é•¿åº¦: {len(content[:2000])} å­—ç¬¦")
            
            # åˆ›å»ºLLMå®ä¾‹ï¼Œæ·»åŠ æ­£ç¡®çš„base_url
            llm = ChatOpenAI(
                model="hunyuan-turbos-latest",  # ä½¿ç”¨hunyuan-turboSæ¨¡å‹
                temperature=0.3,
                api_key=api_key,
                max_tokens=150,
                base_url="https://api.hunyuan.cloud.tencent.com/v1"  # æ·»åŠ æ··å…ƒAPIçš„base_url
            )
            
            # åˆ›å»ºæç¤ºæ¨¡æ¿ï¼Œè¦æ±‚è¿”å›JSONæ ¼å¼
            prompt = PromptTemplate(
                input_variables=["content"],
                template="""è¯·å¯¹ä»¥ä¸‹æ–°é—»å†…å®¹è¿›è¡Œç®€æ´æ¦‚è¿°ï¼Œå¹¶åˆ¤æ–­æ˜¯å¦ä¸ç§‘æŠ€ç›¸å…³ï¼ˆåŒ…æ‹¬AIã€äº’è”ç½‘ã€è½¯ä»¶ã€ç¡¬ä»¶ã€ç”µå­äº§å“ç­‰ï¼‰ã€‚
                
                    æ–°é—»å†…å®¹ï¼š
                    {content}

                    è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
                    1. summary: æ–°é—»æ‘˜è¦ï¼Œä¸è¶…è¿‡50ä¸ªå­—
                    2. is_tech: å¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦ä¸ç§‘æŠ€ç›¸å…³

                    åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦æœ‰ä»»ä½•é¢å¤–è¯´æ˜ã€‚
                    """
            )
            
            # åˆ›å»ºLLMChain
            chain = LLMChain(llm=llm, prompt=prompt)
            
            # è°ƒç”¨æ¨¡å‹
            response = chain.invoke({"content": content[:2000]})  # é™åˆ¶è¾“å…¥é•¿åº¦
            
            result_text = response.get("text", "").strip()
            
            # å°è¯•è§£æJSON
            try:
                # å¦‚æœè¿”å›çš„ä¸æ˜¯çº¯JSONï¼Œå°è¯•æå–JSONéƒ¨åˆ†
                if not result_text.startswith("{"):
                    import re
                    json_match = re.search(r'({.*})', result_text, re.DOTALL)
                    if json_match:
                        result_text = json_match.group(1)
                
                result = json.loads(result_text)
                
                # ç¡®ä¿ç»“æœåŒ…å«å¿…è¦çš„å­—æ®µ
                if "summary" not in result:
                    result["summary"] = ""
                if "is_tech" not in result:
                    result["is_tech"] = False
                
                logger.info(f"ç”Ÿæˆçš„æ‘˜è¦: {result['summary']}, ç§‘æŠ€ç›¸å…³: {result['is_tech']}")
                
                # å¦‚æœå¯ç”¨ç¼“å­˜ï¼Œå°†ç»“æœä¿å­˜åˆ°ç¼“å­˜
                if use_cache and content_hash:
                    summary_cache = load_summary_cache()
                    summary_cache[content_hash] = result
                    save_summary_cache(summary_cache)
                
                return result
            except json.JSONDecodeError:
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œè¿”å›æ–‡æœ¬ä½œä¸ºæ‘˜è¦
                logger.warning(f"JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬: {result_text}")
                result = {"summary": result_text[:50], "is_tech": False}
                
                # å¦‚æœå¯ç”¨ç¼“å­˜ï¼Œå°†ç»“æœä¿å­˜åˆ°ç¼“å­˜
                if use_cache and content_hash:
                    summary_cache = load_summary_cache()
                    summary_cache[content_hash] = result
                    save_summary_cache(summary_cache)
                
                return result
        
        except Exception as e:
            logger.error(f"è°ƒç”¨è…¾è®¯æ··å…ƒæ¨¡å‹å¤±è´¥: {str(e)}")
            retry_count += 1
            if retry_count < max_retries:
                logger.warning(f"5ç§’åé‡è¯• ({retry_count}/{max_retries})...")
                time.sleep(5)
            else:
                break
    
    return {"summary": "", "is_tech": False}


def extract_publish_time_from_html(html_content, url):
    """
    ä»HTMLå†…å®¹ä¸­æå–å‘å¸ƒæ—¶é—´
    æ”¯æŒå¤šç§å¸¸è§çš„æ—¶é—´æ ¼å¼å’ŒHTMLç»“æ„
    """
    if not html_content:
        return None
    
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # 1. å°è¯•ä»metaæ ‡ç­¾ä¸­æå–æ—¶é—´
        meta_tags = [
            soup.find('meta', property='article:published_time'),
            soup.find('meta', property='og:published_time'),
            soup.find('meta', property='publish_date'),
            soup.find('meta', itemprop='datePublished'),
            soup.find('meta', name='pubdate'),
            soup.find('meta', name='publishdate'),
            soup.find('meta', name='date')
        ]
        
        for tag in meta_tags:
            if tag and tag.get('content'):
                try:
                    return date_parser.parse(tag.get('content'))
                except:
                    pass
        
        # 2. å°è¯•ä»timeæ ‡ç­¾ä¸­æå–
        time_tags = soup.find_all('time')
        for time_tag in time_tags:
            datetime_attr = time_tag.get('datetime')
            if datetime_attr:
                try:
                    return date_parser.parse(datetime_attr)
                except:
                    pass
        
        # 3. é’ˆå¯¹ç‰¹å®šç½‘ç«™çš„è‡ªå®šä¹‰æå–é€»è¾‘
        if 'juejin.cn' in url:
            # æ˜é‡‘ç½‘ç«™çš„æ—¶é—´æå–
            time_elements = soup.find_all('time', class_='time')
            for time_element in time_elements:
                if time_element.get('datetime'):
                    try:
                        return date_parser.parse(time_element.get('datetime'))
                    except:
                        pass
        
        # 4. å°è¯•ä»å¸¸è§çš„æ—¥æœŸæ ¼å¼ä¸­æå–
        date_patterns = [
            r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}',  # 2024-03-08 12:34:56
            r'\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}',  # 2024-03-08T12:34:56
            r'\d{4}/\d{2}/\d{2} \d{2}:\d{2}',        # 2024/03/08 12:34
            r'\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥ \d{1,2}:\d{1,2}',  # 2024å¹´3æœˆ8æ—¥ 12:34
            r'\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',            # 2024å¹´3æœˆ8æ—¥
            r'\d{4}-\d{2}-\d{2}'                     # 2024-03-08
        ]
        
        for pattern in date_patterns:
            matches = re.findall(pattern, html_content)
            if matches:
                try:
                    return date_parser.parse(matches[0])
                except:
                    pass
        
        logger.debug(f"æ— æ³•ä»HTMLå†…å®¹ä¸­æå–å‘å¸ƒæ—¶é—´: {url}")
        return None
    
    except Exception as e:
        logger.warning(f"æå–å‘å¸ƒæ—¶é—´æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}, URL: {url}")
        return None


# ä¿®æ”¹process_hotspot_with_summaryå‡½æ•°ï¼Œå¤„ç†ä»HTMLä¸­æå–çš„æ—¶é—´
# ä¿®æ”¹process_hotspot_with_summaryå‡½æ•°ï¼Œä¼ é€’use_cacheå‚æ•°
# ä¿®æ”¹process_hotspot_with_summaryå‡½æ•°ï¼Œæ·»åŠ æ›´æ–°mergedæ–‡ä»¶çš„åŠŸèƒ½
async def process_hotspot_with_summary(hotspots, hunyuan_api_key, max_workers=5, tech_only=False, use_cache=True):
    """
    å¼‚æ­¥å¤„ç†çƒ­ç‚¹æ•°æ®ï¼Œè·å–ç½‘é¡µå†…å®¹å¹¶ç”Ÿæˆæ‘˜è¦
    ä¼˜å…ˆä½¿ç”¨APIè¿”å›çš„æ‘˜è¦ï¼Œæ²¡æœ‰æ‘˜è¦æ—¶æ‰è°ƒç”¨æ··å…ƒæ¨¡å‹
    åŒæ—¶å°è¯•ä»ç½‘é¡µå†…å®¹ä¸­æå–å‘å¸ƒæ—¶é—´
    å¦‚æœtech_onlyä¸ºTrueï¼Œåˆ™åªä¿ç•™ç§‘æŠ€ç›¸å…³çš„å†…å®¹
    æ”¯æŒç¼“å­˜æœºåˆ¶ï¼Œé¿å…é‡å¤å¤„ç†ç›¸åŒå†…å®¹
    å¤„ç†åç›´æ¥æ›´æ–°mergedæ–‡ä»¶
    """
    enhanced_hotspots = []
    
    # è·å–åŸå§‹mergedæ–‡ä»¶è·¯å¾„
    merged_file_path = None
    if hotspots and len(hotspots) > 0 and "saved_at" in hotspots[0]:
        saved_time = hotspots[0]["saved_at"]
        try:
            # ä»saved_atå­—æ®µæå–æ—¶é—´æˆ³
            dt = datetime.fromisoformat(saved_time.replace('Z', '+00:00'))
            date_str = dt.strftime("%Y-%m-%d")
            time_str = dt.strftime("%H-%M-%S")
            merged_file_path = os.path.join("data", "merged", f"hotspots_{date_str}_{time_str}.jsonl")
            logger.info(f"æ‰¾åˆ°åŸå§‹mergedæ–‡ä»¶: {merged_file_path}")
        except Exception as e:
            logger.warning(f"æ— æ³•ä»saved_atæå–æ—¶é—´æˆ³: {str(e)}")
    
    async def process_single_item(item):
        url = item["url"]
        logger.info(f"å¼€å§‹å¤„ç†: {item['title']} ({url})")
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ‘˜è¦
        has_summary = item.get("desc") and len(item.get("desc", "").strip()) > 20
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ—¶é—´æˆ³
        has_timestamp = item.get("timestamp") or item.get("time", "")
        
        # å¦‚æœåŒæ—¶æœ‰æ‘˜è¦å’Œæ—¶é—´æˆ³ï¼Œç›´æ¥ä½¿ç”¨
        if has_summary and has_timestamp:
            logger.info(f"ä½¿ç”¨APIè¿”å›çš„æ‘˜è¦å’Œæ—¶é—´æˆ³: {item['title']}")
            # é»˜è®¤ä¸çŸ¥é“æ˜¯å¦ç§‘æŠ€ç›¸å…³ï¼Œè®¾ä¸ºTrueä»¥é¿å…è¿‡æ»¤
            return {**item, "content": "", "summary": item["desc"], "is_tech": True, "is_processed": True}
        
        # è·å–ç½‘é¡µå†…å®¹å’ŒåŸå§‹HTML
        content, html_content = fetch_webpage_content(url)
        summary_result = {"summary": "", "is_tech": False}
        
        # å¦‚æœæ²¡æœ‰æ—¶é—´æˆ³ï¼Œå°è¯•ä»HTMLä¸­æå–
        if not has_timestamp and html_content:
            publish_time = extract_publish_time_from_html(html_content, url)
            if publish_time:
                logger.info(f"ä»HTMLä¸­æå–åˆ°å‘å¸ƒæ—¶é—´: {publish_time}, æ ‡é¢˜: {item['title']}")
                # æ·»åŠ æå–åˆ°çš„æ—¶é—´æˆ³
                item["extracted_time"] = publish_time.isoformat()
                item["timestamp"] = int(publish_time.timestamp() * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’çº§æ—¶é—´æˆ³
        
        # å¦‚æœæ²¡æœ‰æ‘˜è¦ä½†æœ‰å†…å®¹ï¼Œç”Ÿæˆæ‘˜è¦
        if not has_summary and content:
            summary_result = summarize_with_tencent_hunyuan(content, hunyuan_api_key, use_cache=use_cache)
        elif has_summary:
            # å¦‚æœå·²æœ‰æ‘˜è¦ï¼Œé»˜è®¤è®¾ç½®ä¸ºç§‘æŠ€ç›¸å…³ï¼ˆé¿å…è¿‡æ»¤ï¼‰
            summary_result = {"summary": item["desc"], "is_tech": True}
            
        result = {
            **item, 
            "content": content, 
            "summary": summary_result["summary"],
            "is_tech": summary_result["is_tech"],
            "is_processed": True  # æ·»åŠ å¤„ç†æ ‡è®°
        }
        
        logger.info(f"å¤„ç†å®Œæˆ: {item['title']}, æ‘˜è¦é•¿åº¦: {len(result['summary'])}, ç§‘æŠ€ç›¸å…³: {result['is_tech']}")
        return result
    
    # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œç½‘é¡µå†…å®¹è·å–å’Œæ‘˜è¦ç”Ÿæˆ
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        loop = asyncio.get_event_loop()
        tasks = [
            loop.run_in_executor(
                executor,
                lambda i=i: process_single_item(i)
            )
            for i in hotspots
        ]
        
        completed_tasks = await asyncio.gather(*tasks)
        for completed_task in completed_tasks:
            result = await completed_task
            # å¦‚æœtech_onlyä¸ºTrueï¼Œåªä¿ç•™ç§‘æŠ€ç›¸å…³çš„å†…å®¹
            if not tech_only or result.get("is_tech", False):
                enhanced_hotspots.append(result)
    
    # è®°å½•å¤„ç†ç»“æœç»Ÿè®¡
    with_summary = sum(1 for item in enhanced_hotspots if item.get("summary"))
    with_timestamp = sum(1 for item in enhanced_hotspots if item.get("timestamp") or item.get("time") or item.get("extracted_time"))
    tech_related = sum(1 for item in enhanced_hotspots if item.get("is_tech", False))
    
    logger.info(f"çƒ­ç‚¹å¤„ç†å®Œæˆ: æ€»è®¡ {len(enhanced_hotspots)} æ¡, æˆåŠŸç”Ÿæˆæ‘˜è¦ {with_summary} æ¡, æœ‰æ—¶é—´æˆ³ {with_timestamp} æ¡, ç§‘æŠ€ç›¸å…³ {tech_related} æ¡")
    
    # å¦‚æœæ‰¾åˆ°äº†åŸå§‹mergedæ–‡ä»¶ï¼Œç›´æ¥æ›´æ–°
    if merged_file_path and os.path.exists(merged_file_path):
        try:
            # åˆ›å»ºä¸€ä¸ªIDåˆ°å¤„ç†ç»“æœçš„æ˜ å°„
            processed_items = {item["url"]: item for item in enhanced_hotspots}
            
            # è¯»å–åŸå§‹æ–‡ä»¶å¹¶æ›´æ–°
            updated_lines = []
            with open(merged_file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        item = json.loads(line)
                        url = item.get("url", "")
                        if url in processed_items:
                            # æ›´æ–°å·²å¤„ç†çš„é¡¹ç›®
                            updated_item = processed_items[url]
                            # åªä¿ç•™éœ€è¦çš„å­—æ®µï¼Œä¸åŒ…æ‹¬contentç­‰å¤§å­—æ®µ
                            updated_item_clean = {k: v for k, v in updated_item.items() 
                                               if k not in ["content"]}
                            updated_lines.append(json.dumps(updated_item_clean, ensure_ascii=False))
                        else:
                            # ä¿æŒåŸæœ‰é¡¹ç›®ä¸å˜
                            updated_lines.append(line.strip())
            
            # å†™å›æ–‡ä»¶
            with open(merged_file_path, 'w', encoding='utf-8') as f:
                for line in updated_lines:
                    f.write(line + '\n')
            
            logger.info(f"å·²æ›´æ–°mergedæ–‡ä»¶: {merged_file_path}")
        except Exception as e:
            logger.error(f"æ›´æ–°mergedæ–‡ä»¶å¤±è´¥: {str(e)}")
    
    return enhanced_hotspots
    

# ä¿®æ”¹summarize_with_deepseekå‡½æ•°ï¼Œæ·»åŠ ä¿å­˜JSONæ•°æ®çš„åŠŸèƒ½
def summarize_with_deepseek(hotspots, api_key, api_url=None, model_id=None, max_retries=3, tech_only=False):
    """
    ä½¿ç”¨Deepseek APIå¯¹çƒ­ç‚¹è¿›è¡Œæ±‡æ€»å½’ç±»ï¼Œæ”¯æŒé‡è¯•
    æ ¹æ®tech_onlyå‚æ•°ä½¿ç”¨ä¸åŒçš„prompt
    """
    if api_url is None:
        api_url = "https://ark.cn-beijing.volces.com/api/v3/chat/completions"
    
    if model_id is None:
        model_id = "ep-20250220195531-ds6nm"
    
    retry_count = 0
    while retry_count < max_retries:
        try:
            # ç®€åŒ–è¾“å…¥æ•°æ®ï¼Œåªä¼ é€’å¿…è¦ä¿¡æ¯ï¼Œä½†åŒ…å«æ‘˜è¦
            simplified_hotspots = []
            for idx, item in enumerate(hotspots):
                source_name = SOURCE_NAME_MAP.get(item['source'], item['source'])
                simplified_hotspots.append({
                    "id": idx,
                    "title": item['title'],
                    "source": source_name,
                    "summary": item.get('summary', '')  # æ·»åŠ æ‘˜è¦ä¿¡æ¯
                })
            
            # å°†å®Œæ•´æ•°æ®è½¬æ¢ä¸ºå­—å…¸ä»¥ä¾¿åç»­æŸ¥æ‰¾
            hotspot_dict = {idx: item for idx, item in enumerate(hotspots)}
            
            # è½¬æ¢ä¸ºJSONæ ¼å¼çš„è¾“å…¥
            hotspot_json = json.dumps(simplified_hotspots, ensure_ascii=False)
            
            # ä¿å­˜è¾“å…¥çš„JSONæ•°æ®ï¼Œä½¿ç”¨ç›¸å¯¹è·¯å¾„
            save_directory = os.path.join("data", "inputs")
            os.makedirs(save_directory, exist_ok=True)
            today = datetime.now().strftime("%Y-%m-%d")
            timestamp = datetime.now().strftime("%H-%M-%S")
            input_filename = os.path.join(save_directory, f"deepseek_input_{today}_{timestamp}.json")
            with open(input_filename, 'w', encoding='utf-8') as f:
                f.write(hotspot_json)
            logger.info(f"å·²ä¿å­˜Deepseekè¾“å…¥æ•°æ®è‡³ {input_filename}")
            
            # æ ¹æ®tech_onlyå‚æ•°é€‰æ‹©ä¸åŒçš„prompt
            if tech_only:
                prompt = f"""
                ä»¥ä¸‹æ˜¯ä»Šæ—¥ç§‘æŠ€çƒ­ç‚¹æ–°é—»åˆ—è¡¨ï¼ˆJSONæ ¼å¼ï¼‰ï¼Œæ¯ä¸ªæ¥æºå‡å·²æŒ‰ç…§çƒ­æ¦œæ’åºï¼Œéƒ¨åˆ†æ–°é—»åŒ…å«å†…å®¹æ‘˜è¦ï¼š
                {hotspot_json}
                è¯·æ€»ç»“å‡º10æ¡æœ€é‡è¦çš„ç§‘æŠ€æ–°é—»ï¼Œä¼˜å…ˆé€‰æ‹©AIç›¸å…³æ–°é—»ï¼Œå»é™¤é‡å¤å’Œæ— å…³å†…å®¹ã€‚AIç›¸å…³æ–°é—»æ’åºä¼˜å…ˆé å‰ï¼Œå…¬ä¼—å·çš„æ–‡ç« æƒé‡æ›´é«˜ï¼Œå…¶ä½™æŒ‰é‡è¦æ€§æ’åºã€‚
                ä½ éœ€è¦å°†ç›¸ä¼¼çš„æ–°é—»åˆå¹¶ä¸ºä¸€æ¡ï¼Œå¹¶æä¾›ä¸€ä¸ªç›´è§‚ç®€æ´çš„æ ‡é¢˜ï¼Œéœ€è¦è®²æ¸…æ¥šæ–°é—»å†…å®¹ä¸è¦å¤ªæ³›åŒ–ï¼ˆä¸è¶…è¿‡30ä¸ªå­—ï¼‰ã€‚
                ç›¸å…³æ–°é—»çš„IDåˆ—è¡¨æœ€å¤šé€‰æ‹©å…¶ä¸­4æ¡ï¼ˆå–æœ€å…¸å‹çš„ï¼‰ï¼Œè¶…è¿‡æ•°é‡ä¸éœ€è¦å…¨éƒ¨ç»™å‡ºã€‚
                å¦‚æœæœ‰æ‘˜è¦ä¿¡æ¯ï¼Œè¯·å‚è€ƒæ‘˜è¦æä¾›æ›´å‡†ç¡®çš„æ ‡é¢˜ã€‚
                
                è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
                ```json
                [
                  {{
                    "title": "æ–°é—»æ ‡é¢˜",
                    "related_ids": [ç›¸å…³æ–°é—»çš„IDåˆ—è¡¨]
                  }},
                  ...
                ]
                ```
                
                åªè¿”å›JSONæ•°æ®ï¼Œä¸è¦æœ‰ä»»ä½•é¢å¤–è¯´æ˜ã€‚
                """
            else:
                prompt = f"""
                ä»¥ä¸‹æ˜¯ä»Šæ—¥çƒ­ç‚¹æ–°é—»åˆ—è¡¨ï¼ˆJSONæ ¼å¼ï¼‰ï¼Œæ¯ä¸ªæ¥æºå‡å·²æŒ‰ç…§çƒ­æ¦œæ’åºï¼Œéƒ¨åˆ†æ–°é—»åŒ…å«å†…å®¹æ‘˜è¦ï¼š
                {hotspot_json}
                è¯·æ€»ç»“å‡º10æ¡æœ€é‡è¦çš„çƒ­ç‚¹æ–°é—»ï¼Œä¼˜å…ˆé€‰æ‹©ç§‘æŠ€å’ŒAIç›¸å…³æ–°é—»ï¼Œä½†ä¹Ÿè¦åŒ…å«å…¶ä»–é¢†åŸŸï¼ˆå¦‚ç¤¾ä¼šã€å¨±ä¹ã€ä½“è‚²ç­‰ï¼‰çš„é‡è¦æ–°é—»ï¼Œå»é™¤é‡å¤å†…å®¹ã€‚
                ä½ éœ€è¦å°†ç›¸ä¼¼çš„æ–°é—»åˆå¹¶ä¸ºä¸€æ¡ï¼Œå¹¶æä¾›ä¸€ä¸ªç›´è§‚ç®€æ´çš„æ ‡é¢˜ï¼Œéœ€è¦è®²æ¸…æ¥šæ–°é—»å†…å®¹ä¸è¦å¤ªæ³›åŒ–ï¼ˆä¸è¶…è¿‡30ä¸ªå­—ï¼‰ã€‚
                ç›¸å…³æ–°é—»çš„IDåˆ—è¡¨æœ€å¤šé€‰æ‹©å…¶ä¸­4æ¡ï¼ˆå–æœ€å…¸å‹çš„ï¼‰ï¼Œè¶…è¿‡æ•°é‡ä¸éœ€è¦å…¨éƒ¨ç»™å‡ºã€‚
                å¦‚æœæœ‰æ‘˜è¦ä¿¡æ¯ï¼Œè¯·å‚è€ƒæ‘˜è¦æä¾›æ›´å‡†ç¡®çš„æ ‡é¢˜ã€‚
                
                è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
                ```json
                [
                  {{
                    "title": "æ–°é—»æ ‡é¢˜",
                    "related_ids": [ç›¸å…³æ–°é—»çš„IDåˆ—è¡¨]
                  }},
                  ...
                ]
                ```
                
                åªè¿”å›JSONæ•°æ®ï¼Œä¸è¦æœ‰ä»»ä½•é¢å¤–è¯´æ˜ã€‚
                """
            
            # è°ƒç”¨Deepseek API
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key}"
            }
            
            payload = {
                "model": model_id,
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»ç¼–è¾‘åŠ©æ‰‹ï¼Œæ“…é•¿å½’çº³æ€»ç»“çƒ­ç‚¹æ–°é—»ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.3,
                "max_tokens": 1000
            }
            
            logger.info(f"æ­£åœ¨è°ƒç”¨ Deepseek APIï¼Œå°è¯•æ¬¡æ•°: {retry_count + 1}/{max_retries}")
            response = requests.post(
                api_url,
                headers=headers,
                json=payload,
                timeout=60
            )
            
            response.raise_for_status()
            result = response.json()
            
            # æå–å›å¤å†…å®¹
            json_response = result["choices"][0]["message"]["content"]
            
            # æå–JSONéƒ¨åˆ†
            json_str = json_response
            if "```json" in json_response:
                json_str = json_response.split("```json")[1].split("```")[0].strip()
            
            # ä¿å­˜Deepseekçš„è¾“å‡ºç»“æœï¼Œä½¿ç”¨ç›¸å¯¹è·¯å¾„
            output_directory = os.path.join("data", "outputs")
            os.makedirs(output_directory, exist_ok=True)
            output_filename = os.path.join(output_directory, f"deepseek_output_{today}_{timestamp}.json")
            with open(output_filename, 'w', encoding='utf-8') as f:
                f.write(json_str)
            logger.info(f"å·²ä¿å­˜Deepseekè¾“å‡ºæ•°æ®è‡³ {output_filename}")
            
            

            # ä¿å­˜Deepseekçš„å®Œæ•´å“åº”ç»“æœ
            output_directory = os.path.join("data", "outputs")
            os.makedirs(output_directory, exist_ok=True)
            
            # ä¿å­˜åŸå§‹å“åº”
            raw_output_filename = os.path.join(output_directory, f"deepseek_raw_response_{today}_{timestamp}.json")
            with open(raw_output_filename, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            logger.info(f"å·²ä¿å­˜DeepseekåŸå§‹å“åº”è‡³ {raw_output_filename}")
            
            # ä¿å­˜å¤„ç†åçš„JSONè¾“å‡º
            output_filename = os.path.join(output_directory, f"deepseek_output_{today}_{timestamp}.json")
            with open(output_filename, 'w', encoding='utf-8') as f:
                f.write(json_str)
            logger.info(f"å·²ä¿å­˜Deepseekè¾“å‡ºæ•°æ®è‡³ {output_filename}")
            
            # è§£æJSON
            try:
                news_items = json.loads(json_str)
                
                # æ ¹æ®JSONæ„å»ºæœ€ç»ˆè¾“å‡º
                formatted_summary = ""
                for index, news in enumerate(news_items[:20]):
                    num = str(index + 1).zfill(2)
                    title = news.get("title", "æœªçŸ¥æ ‡é¢˜")
                    
                    formatted_summary += f"## ** {num} {title} **  \n"
                    
                    # æ·»åŠ ç›¸å…³é“¾æ¥ï¼Œä½¿ç”¨ä¼˜åŒ–çš„æ ¼å¼
                    related_ids = news.get("related_ids", [])
                    for news_id in related_ids:
                        if news_id in hotspot_dict:
                            item = hotspot_dict[news_id]
                            source_name = SOURCE_NAME_MAP.get(item['source'], item['source'])
                            item_title = item['title']
                            # æ ¼å¼åŒ–æ ‡é¢˜ï¼Œç¡®ä¿é•¿åº¦ä¸€è‡´
                            if len(item_title) > 18:
                                item_title = item_title[:15] + "..."
                            
                            # æ·»åŠ é“¾æ¥
                            formatted_summary += f"- [{item_title}]({item['url']}) `ğŸ·ï¸{source_name}`\n"
                    
                    # æ·»åŠ ç©ºè¡Œåˆ†éš”
                    formatted_summary += "\n"
                
                # ä¿å­˜æ ¼å¼åŒ–åçš„æ‘˜è¦å†…å®¹
                summary_filename = os.path.join(output_directory, f"formatted_summary_{today}_{timestamp}.md")
                with open(summary_filename, 'w', encoding='utf-8') as f:
                    f.write(formatted_summary)
                logger.info(f"å·²ä¿å­˜æ ¼å¼åŒ–æ‘˜è¦è‡³ {summary_filename}")
                
                return formatted_summary
                
            except json.JSONDecodeError as e:
                logger.error(f"è§£æDeepseekè¿”å›çš„JSONå¤±è´¥: {str(e)}")
                return f"è§£æDeepseekè¿”å›çš„JSONå¤±è´¥: {str(e)}"
            except json.JSONDecodeError as e:
                logger.error(f"JSONè§£æé”™è¯¯: {e}")
                logger.error(f"åŸå§‹JSONå­—ç¬¦ä¸²: {json_str}")
                raise
            
        except requests.exceptions.Timeout:
            retry_count += 1
            logger.warning(f"Deepseek API è¯·æ±‚è¶…æ—¶ï¼Œæ­£åœ¨é‡è¯• ({retry_count}/{max_retries})...")
            time.sleep(5)  # ç­‰å¾…5ç§’åé‡è¯•
        
        except Exception as e:
            logger.error(f"è°ƒç”¨Deepseek APIæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            retry_count += 1
            if retry_count < max_retries:
                logger.warning(f"5ç§’åé‡è¯• ({retry_count}/{max_retries})...")
                time.sleep(5)
            else:
                break
    
    # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å›å‰20æ¡çƒ­ç‚¹ä½œä¸ºå¤‡é€‰
    logger.warning("æ— æ³•ä½¿ç”¨Deepseek APIå½’ç±»çƒ­ç‚¹ï¼Œå°†ä½¿ç”¨åŸå§‹çƒ­ç‚¹")
    fallback = ""
    for i, item in enumerate(hotspots[:10]):
        num = str(i + 1).zfill(2)
        source_name = SOURCE_NAME_MAP.get(item['source'], item['source'])
        item_title = item['title']
        # æ ¼å¼åŒ–æ ‡é¢˜ï¼Œç¡®ä¿é•¿åº¦ä¸€è‡´
        formatted_title = format_title_for_display(item_title, source_name, 30)
        fallback += f"## ** {num} {item['title']} **  \n"
        fallback += f"- [{item_title}]({item['url']}) `ğŸ·ï¸{source_name}` \n\n"
    return fallback



def send_to_webhook(webhook_url, content, is_tech_only=False):
    """
    å°†å†…å®¹å‘é€åˆ°webhookÂ·
    ä¿å­˜å‘é€çš„å†…å®¹åˆ°æ–‡ä»¶
    """
    try:
        # è·å–å½“å‰æ—¥æœŸå’Œæ—¶é—´
        now = datetime.now()
        today = now.strftime("%Y-%m-%d")
        current_time = now.strftime("%H:%M")
        
        # æ ¹æ®æ˜¯å¦åªåŒ…å«ç§‘æŠ€çƒ­ç‚¹æ¥è®¾ç½®æ ‡é¢˜
        title_prefix = "ç§‘æŠ€çƒ­ç‚¹" if is_tech_only else "çƒ­ç‚¹æ–°é—»"
        
        # æ·»åŠ æ ‡é¢˜å’ŒæŸ¥çœ‹å…¨éƒ¨çƒ­ç‚¹çš„é“¾æ¥
        header = f"# {today} {current_time} {title_prefix}æ—©æŠ¥\n\n"
        footer = f"\n\n[æŸ¥çœ‹å…¨éƒ¨çƒ­ç‚¹](https://hot.tuber.cc/)"
        
        # æ„å»ºä¼ä¸šå¾®ä¿¡markdownæ ¼å¼çš„å†…å®¹
        markdown_content = header + content + footer
        
        # ä¿å­˜å‘é€åˆ°webhookçš„å†…å®¹
        output_directory = os.path.join("data", "webhook")
        os.makedirs(output_directory, exist_ok=True)
        timestamp = now.strftime("%H-%M-%S")
        webhook_filename = os.path.join(output_directory, f"webhook_content_{today}_{timestamp}.md")
        with open(webhook_filename, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        logger.info(f"å·²ä¿å­˜webhookå‘é€å†…å®¹è‡³ {webhook_filename}")
        
        # ä¼ä¸šå¾®ä¿¡webhookæ ¼å¼
        payload = {
            "msgtype": "markdown",
            "markdown": {
                "content": markdown_content
            }
        }
        
        # ä¿å­˜å®Œæ•´çš„webhookè¯·æ±‚
        webhook_request_filename = os.path.join(output_directory, f"webhook_request_{today}_{timestamp}.json")
        with open(webhook_request_filename, 'w', encoding='utf-8') as f:
            json.dump(payload, f, ensure_ascii=False, indent=2)
        logger.info(f"å·²ä¿å­˜webhookè¯·æ±‚æ•°æ®è‡³ {webhook_request_filename}")
        
        response = requests.post(webhook_url, json=payload, timeout=10)
        response.raise_for_status()
        
        # ä¿å­˜webhookå“åº”
        webhook_response_filename = os.path.join(output_directory, f"webhook_response_{today}_{timestamp}.json")
        try:
            response_data = response.json()
            with open(webhook_response_filename, 'w', encoding='utf-8') as f:
                json.dump(response_data, f, ensure_ascii=False, indent=2)
        except:
            with open(webhook_response_filename, 'w', encoding='utf-8') as f:
                f.write(f"Status Code: {response.status_code}\nContent: {response.text}")
        logger.info(f"å·²ä¿å­˜webhookå“åº”è‡³ {webhook_response_filename}")
        
        logger.info(f"æˆåŠŸæ¨é€{title_prefix}åˆ°webhook")
        return True
    except Exception as e:
        logger.error(f"æ¨é€åˆ°webhookæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return False


def filter_recent_hotspots(hotspots, days=1):
    """
    ç­›é€‰æ—¶é—´èŒƒå›´å†…çš„çƒ­ç‚¹æ•°æ®
    æ—¶é—´èŒƒå›´ï¼šæ˜¨å¤©æ•´å¤© + ä»Šå¤©åˆ°å½“å‰æ—¶é—´
    """
    filtered_hotspots = []
    current_time = datetime.now()
    
    # è®¾ç½®æ—¶é—´èŒƒå›´ï¼šæ˜¨å¤©0ç‚¹åˆ°ç°åœ¨
    yesterday = current_time.replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(days=1)
    
    logger.info(f"å½“å‰æ—¶é—´: {current_time}, ç­›é€‰æ—¶é—´èŒƒå›´: {yesterday} è‡³ {current_time}")
    
    for item in hotspots:
        # å°è¯•è§£ææ—¶é—´æˆ³
        timestamp = item.get("timestamp") or item.get("time", "")
        
        if timestamp:
            try:
                # å°†æ—¶é—´æˆ³è½¬æ¢ä¸ºdatetimeå¯¹è±¡
                if isinstance(timestamp, str):
                    if 'T' in timestamp and ('Z' in timestamp or '+' in timestamp):
                        # ISOæ ¼å¼: 2025-03-08T12:04:22.020Z
                        item_time = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                    else:
                        # å°è¯•ä½œä¸ºæ•°å­—å¤„ç†
                        timestamp = float(timestamp)
                        # æ¯«ç§’çº§æ—¶é—´æˆ³è½¬æ¢ä¸ºç§’çº§æ—¶é—´æˆ³
                        if timestamp > 9999999999:
                            timestamp = timestamp / 1000
                        item_time = datetime.fromtimestamp(timestamp)
                else:
                    # æ•°å­—ç±»å‹æ—¶é—´æˆ³
                    if timestamp > 9999999999:  # æ¯«ç§’çº§æ—¶é—´æˆ³
                        timestamp = timestamp / 1000
                    item_time = datetime.fromtimestamp(float(timestamp))
                
                # æ£€æŸ¥æ—¶é—´æ˜¯å¦åœ¨æœªæ¥ï¼ˆå¯èƒ½æ˜¯é”™è¯¯çš„æ—¶é—´æˆ³ï¼‰
                if item_time > current_time + timedelta(hours=1):
                    # å¯èƒ½æ˜¯æœªæ¥çš„æ—¶é—´æˆ³ï¼Œå°è¯•è°ƒæ•´å¹´ä»½
                    logger.warning(f"æ£€æµ‹åˆ°æœªæ¥æ—¶é—´æˆ³: {item_time}, æ ‡é¢˜: {item['title']}")
                    
                    # å¦‚æœæ—¶é—´æˆ³å¯¹åº”çš„å¹´ä»½æ˜¯æœªæ¥å¹´ä»½ï¼Œè°ƒæ•´ä¸ºå½“å‰å¹´ä»½
                    if item_time.year > current_time.year:
                        adjusted_year = current_time.year
                        try:
                            item_time = item_time.replace(year=adjusted_year)
                            logger.info(f"è°ƒæ•´æ—¶é—´æˆ³å¹´ä»½ä¸ºå½“å‰å¹´ä»½: {item_time}")
                        except ValueError as e:
                            logger.warning(f"è°ƒæ•´æ—¶é—´æˆ³å¹´ä»½å¤±è´¥: {str(e)}")
                
                # è®°å½•è§£æç»“æœ
                logger.info(f"çƒ­ç‚¹: {item['title'][:30]}..., æ—¶é—´: {item_time}")
                
                # åªä¿ç•™æ˜¨å¤©0ç‚¹åˆ°ç°åœ¨çš„çƒ­ç‚¹
                if yesterday <= item_time <= current_time:
                    filtered_hotspots.append(item)
                    continue
                else:
                    logger.info(f"ä¸¢å¼ƒæ—¶é—´èŒƒå›´å¤–çƒ­ç‚¹: {item['title']}, æ—¶é—´: {item_time}")
                    continue
            except (ValueError, TypeError) as e:
                logger.warning(f"è§£ææ—¶é—´æˆ³å¤±è´¥: {timestamp}, é”™è¯¯: {str(e)}, æ ‡é¢˜: {item['title']}")
        
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„æ—¶é—´æˆ³æˆ–è§£æå¤±è´¥ï¼Œé»˜è®¤ä¿ç•™è¯¥æ¡ç›®
        logger.info(f"æ— æœ‰æ•ˆæ—¶é—´æˆ³ï¼Œé»˜è®¤ä¿ç•™: {item['title']}")
        filtered_hotspots.append(item)
    
    logger.info(f"ç­›é€‰åä¿ç•™ {len(filtered_hotspots)}/{len(hotspots)} æ¡æ—¶é—´èŒƒå›´å†…çš„çƒ­ç‚¹æ•°æ®")
    return filtered_hotspots
def fetch_rss_articles(rss_url, days=1):
    """
    ä»RSSæºè·å–æœ€è¿‘æŒ‡å®šå¤©æ•°å†…çš„æ–‡ç« 
    """
    try:
        logger.info(f"æ­£åœ¨è·å–RSSæº: {rss_url}")
        feed = feedparser.parse(rss_url)
        
        if feed.bozo:  # æ£€æŸ¥feedè§£ææ˜¯å¦æœ‰é”™è¯¯
            logger.warning(f"RSSè§£æè­¦å‘Š: {feed.bozo_exception}")
        
        articles = []
        current_time = datetime.now()
        cutoff_time = current_time - timedelta(days=days)
        
        for entry in feed.entries:
            # å°è¯•è·å–å‘å¸ƒæ—¶é—´
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                pub_time = datetime.fromtimestamp(time.mktime(entry.published_parsed))
            elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                pub_time = datetime.fromtimestamp(time.mktime(entry.updated_parsed))
            else:
                # å¦‚æœæ²¡æœ‰æ—¶é—´ä¿¡æ¯ï¼Œå‡è®¾æ˜¯æœ€è¿‘çš„
                pub_time = current_time
            
            # è·å–ä½œè€…ä¿¡æ¯ä½œä¸ºæ¥æº
            source = "å…¬ä¼—å·ç²¾é€‰"
            if hasattr(entry, 'author') and entry.author:
                source = f"{entry.author}"
            
            # åªä¿ç•™æœ€è¿‘dayså¤©çš„æ–‡ç« 
            if pub_time >= cutoff_time:
                articles.append({
                    "title": entry.title,
                    "url": entry.link,
                    "source": source,
                    "hot": "",
                    "published": pub_time.strftime("%Y-%m-%d %H:%M:%S")
                })
        
        logger.info(f"ä»RSSæºè·å–åˆ° {len(articles)} ç¯‡æœ€è¿‘{days}å¤©çš„æ–‡ç« ")
        return articles
    except Exception as e:
        logger.error(f"è·å–RSSæºæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return []
def main():
    parser = argparse.ArgumentParser(description="çƒ­ç‚¹æ–°é—»æ”¶é›†ä¸æ¨é€å·¥å…·")
    parser.add_argument("--tech-only", action="store_true", help="ä»…æ”¶é›†ç§‘æŠ€ç›¸å…³çš„çƒ­ç‚¹")
    parser.add_argument("--webhook", required=True, help="Webhook URL")
    parser.add_argument("--deepseek-key", required=True, help="Deepseek API Key")
    parser.add_argument("--hunyuan-key", required=True, help="è…¾è®¯æ··å…ƒ API Key")
    parser.add_argument("--no-cache", action="store_true", 
                        help="ç¦ç”¨æ‘˜è¦ç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°ç”Ÿæˆæ‰€æœ‰æ‘˜è¦")
    parser.add_argument("--base-url", default="https://api-hot.tuber.cc", help="DailyHotApi åŸºç¡€URL")
    parser.add_argument("--deepseek-url", default="https://ark.cn-beijing.volces.com/api/v3/chat/completions", 
                        help="Deepseek API URL")
    parser.add_argument("--model-id", default="ep-20250307234946-b2znq", 
                        help="Deepseek Model ID")
    parser.add_argument("--rss-url", default="https://wewe.tuber.cc/feeds/all.atom?limit=20", 
                        help="RSSæºURL")
    parser.add_argument("--rss-days", type=int, default=1, 
                        help="è·å–RSSä¸­æœ€è¿‘å‡ å¤©çš„æ–‡ç« ")
    parser.add_argument("--title-length", type=int, default=20, 
                        help="æ˜¾ç¤ºæ ‡é¢˜çš„æœ€å¤§é•¿åº¦")
    parser.add_argument("--max-workers", type=int, default=5, 
                        help="å¹¶å‘å¤„ç†ç½‘é¡µå†…å®¹çš„æœ€å¤§çº¿ç¨‹æ•°")
    parser.add_argument("--skip-content", action="store_true", 
                        help="è·³è¿‡è·å–ç½‘é¡µå†…å®¹å’Œç”Ÿæˆæ‘˜è¦æ­¥éª¤")
    parser.add_argument("--filter-days", type=int, default=1,
                        help="ç­›é€‰æœ€è¿‘å‡ å¤©çš„çƒ­ç‚¹æ•°æ®")
    args = parser.parse_args()
    
    # æ£€æŸ¥ BASE_URL æ˜¯å¦å¯è®¿é—®
    if not check_base_url(args.base_url):
        logger.error(f"BASE_URL {args.base_url} ä¸å¯è®¿é—®ï¼Œç¨‹åºé€€å‡º")
        sys.exit(1)
    
    # æ ¹æ®å‚æ•°é€‰æ‹©ä¿¡æ¯æº
    sources = TECH_SOURCES if args.tech_only else ALL_SOURCES
    
    # æ”¶é›†çƒ­ç‚¹
    hotspots = collect_all_hotspots(sources, args.base_url)
    
    if not hotspots:
        logger.error("æœªæ”¶é›†åˆ°ä»»ä½•çƒ­ç‚¹æ•°æ®ï¼Œç¨‹åºé€€å‡º")
        sys.exit(1)
    
    # ä¿å­˜åŸå§‹çƒ­ç‚¹æ•°æ®
    save_hotspots_to_jsonl(hotspots)
    
    # ç­›é€‰æœ€è¿‘çš„çƒ­ç‚¹
    hotspots = filter_recent_hotspots(hotspots, args.filter_days)
    
    # ä¿å­˜ç­›é€‰åçš„çƒ­ç‚¹æ•°æ®
    save_hotspots_to_jsonl(hotspots, directory=os.path.join("data", "filtered"))
    
    # è·å–RSSæ–‡ç« 
    rss_articles = fetch_rss_articles(args.rss_url, args.rss_days)
    
    # åˆå¹¶çƒ­ç‚¹å’ŒRSSæ–‡ç« 
    all_content = hotspots + rss_articles
    logger.info(f"åˆå¹¶åå…±æœ‰ {len(all_content)} æ¡å†…å®¹")
    
    # ä¿å­˜åˆå¹¶åçš„æ•°æ®
    save_hotspots_to_jsonl(all_content, directory=os.path.join("data", "merged"))
    
    # è·å–ç½‘é¡µå†…å®¹å¹¶ç”Ÿæˆæ‘˜è¦
    if not args.skip_content:
        try:
            # ç¡®ä¿æœ‰äº‹ä»¶å¾ªç¯
            if asyncio.get_event_loop().is_closed():
                asyncio.set_event_loop(asyncio.new_event_loop())
            
            # ä½¿ç”¨å¼‚æ­¥æ–¹å¼å¤„ç†æ‰€æœ‰å†…å®¹ï¼Œä¼ é€’tech_onlyå‚æ•°å’Œuse_cacheå‚æ•°
            loop = asyncio.get_event_loop()
            all_content_with_summary = loop.run_until_complete(
                process_hotspot_with_summary(all_content, args.hunyuan_key, args.max_workers, 
                                           args.tech_only, use_cache=not args.no_cache)
            )
            logger.info(f"å·²ä¸º {len(all_content_with_summary)} æ¡å†…å®¹ç”Ÿæˆæ‘˜è¦")
        except Exception as e:
            logger.error(f"è·å–ç½‘é¡µå†…å®¹æˆ–ç”Ÿæˆæ‘˜è¦æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            # å¦‚æœå‡ºé”™ï¼Œç»§ç»­ä½¿ç”¨åŸå§‹å†…å®¹
            all_content_with_summary = all_content
    else:
        all_content_with_summary = all_content
        logger.info("å·²è·³è¿‡è·å–ç½‘é¡µå†…å®¹å’Œç”Ÿæˆæ‘˜è¦æ­¥éª¤")
    
    # ä½¿ç”¨Deepseekæ±‡æ€»ï¼Œä¼ é€’tech_onlyå‚æ•°
    summary = summarize_with_deepseek(all_content_with_summary, args.deepseek_key, 
                                     args.deepseek_url, args.model_id, tech_only=args.tech_only)
    
    # å‘é€åˆ°webhook
    send_to_webhook(args.webhook, summary, args.tech_only)
    
    logger.info("å¤„ç†å®Œæˆ")

if __name__ == "__main__":
    main()
