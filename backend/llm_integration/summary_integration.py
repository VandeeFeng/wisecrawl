import os
import json
import time
import logging
import requests
from datetime import datetime
from config.config import SOURCE_NAME_MAP, DEEPSEEK_MODEL_ID
from utils.utils import format_title_for_display, get_project_root
from utils.token_tracker import token_tracker

logger = logging.getLogger(__name__)

def summarize_with_deepseek(hotspots, api_key, api_url=None, model_id=None, max_retries=3, tech_only=False):
    """
    ä½¿ç”¨Deepseek APIå¯¹çƒ­ç‚¹è¿›è¡Œæ±‡æ€»å½’ç±»ï¼Œæ”¯æŒé‡è¯•
    æ ¹æ®tech_onlyå‚æ•°ä½¿ç”¨ä¸åŒçš„prompt
    """
    if api_url is None:
        api_url = "http://127.0.0.1:11434/v1/chat/completions"
    
    if model_id is None:
        model_id = DEEPSEEK_MODEL_ID
    
    retry_count = 0
    while retry_count < max_retries:
        try:
            simplified_hotspots = []
            for idx, item in enumerate(hotspots):
                source_name = SOURCE_NAME_MAP.get(item['source'], item['source'])
                simplified_hotspots.append({
                    "id": idx,
                    "title": item['title'],
                    "source": source_name,
                    "summary": item.get('summary', '')  
                })
            
            hotspot_dict = {idx: item for idx, item in enumerate(hotspots)}
            
            hotspot_json = json.dumps(simplified_hotspots, ensure_ascii=False)
            
            # Get project root directory
            project_root = get_project_root()
            # Build absolute paths from project root
            save_directory = os.path.join(project_root, "data", "inputs")
            os.makedirs(save_directory, exist_ok=True)
            today = datetime.now().strftime("%Y-%m-%d")
            timestamp = datetime.now().strftime("%H-%M-%S")
            input_filename = os.path.join(save_directory, f"deepseek_input_{today}_{timestamp}.json")
            with open(input_filename, 'w', encoding='utf-8') as f:
                f.write(hotspot_json)
            logger.info(f"å·²ä¿å­˜Deepseekè¾“å…¥æ•°æ®è‡³ {input_filename}")
            
            if tech_only:
                prompt = f"""
                ä»¥ä¸‹æ˜¯ä»Šæ—¥ç§‘æŠ€çƒ­ç‚¹ä¿¡æ¯åˆ—è¡¨ï¼ˆåŒ…å«æ–°é—»å’Œç¤¾äº¤åª’ä½“å¸–å­ï¼ŒJSONæ ¼å¼ï¼‰ï¼Œéƒ¨åˆ†æ¡ç›®åŒ…å«å†…å®¹æ‘˜è¦ï¼š
                {hotspot_json}
                è¯·æ€»ç»“å‡º10æ¡æœ€é‡è¦çš„ç§‘æŠ€æ–°é—»ï¼Œä¼˜å…ˆé€‰æ‹©AIç›¸å…³æ–°é—»ï¼Œå»é™¤é‡å¤å’Œæ— å…³å†…å®¹ã€‚
                é‡ç‚¹å…³æ³¨æœ€æ–°å‘å¸ƒçš„AIæŠ€æœ¯æˆ–è€…æ¨¡å‹ç­‰ï¼Œç›¸å…³æ–°é—»åœ¨è¿”å›çš„ç»“æœæ’åºä¸­éœ€è¦å‰ç½®ï¼›å…¬ä¼—å·çš„æ–‡ç« æƒé‡æ›´é«˜ï¼Œå…¶ä½™ç»“æœæŒ‰é‡è¦æ€§æ’åºã€‚
                ä½ éœ€è¦å°†ç›¸ä¼¼çš„æ–°é—»åˆå¹¶ä¸ºä¸€æ¡ï¼Œå¹¶æä¾›ä¸€ä¸ªç›´è§‚ç®€æ´çš„ä¸­æ–‡æ ‡é¢˜ï¼Œéœ€è¦è®²æ¸…æ¥šæ–°é—»å†…å®¹ä¸è¦å¤ªæ³›åŒ–ï¼ˆä¸è¶…è¿‡30ä¸ªå­—ï¼‰ã€‚
                åŒæ—¶ï¼Œä¹Ÿè¯·å…³æ³¨æ¥è‡ª Twitter ç­‰ç¤¾äº¤åª’ä½“æº (source: Twitter) çš„é‡è¦ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯å…³äºæœ€æ–° AI æŠ€æœ¯çªç ´ã€æ¨¡å‹å‘å¸ƒæˆ–é‡è¦è¡Œä¸šåŠ¨æ€çš„å¸–å­ï¼Œå®ƒä»¬åŒæ ·å…·æœ‰å¾ˆé«˜çš„ä»·å€¼ã€‚
                ç›¸å…³æ–°é—»çš„IDåˆ—è¡¨æœ€å¤šé€‰æ‹©å…¶ä¸­4æ¡ï¼Œå–æœ€å…¸å‹çš„ï¼Œè¶…è¿‡æ•°é‡ä¸éœ€è¦å…¨éƒ¨ç»™å‡ºã€‚è¯·ç‰¹åˆ«æ³¨æ„ï¼Œå¦‚æœåŒä¸€å®¶åª’ä½“åœ¨å¤šä¸ªæ¸ é“å‘å¸ƒç›¸åŒçš„å†…å®¹ï¼Œæˆ–æ–°é—»æ ‡é¢˜ç›¸ä¼¼åº¦æé«˜ï¼Œä¸è¦åŒæ—¶é€‰æ‹©ï¼Œåˆ™ä»…éœ€åˆ—å‡º1æ¡å³å¯ã€‚
                å¦‚æœæœ‰æ‘˜è¦ä¿¡æ¯ï¼Œè¯·å‚è€ƒæ‘˜è¦æä¾›æ›´å‡†ç¡®çš„æ ‡é¢˜ã€‚
                
                è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
                ```json
                [
                  {{
                    "title": "çƒ­ç‚¹æ ‡é¢˜",
                    "related_ids": [ç›¸å…³çƒ­ç‚¹çš„IDåˆ—è¡¨]
                  }},
                  ...
                ]
                ```
                
                åªè¿”å›JSONæ•°æ®ï¼Œä¸è¦æœ‰ä»»ä½•é¢å¤–è¯´æ˜ã€‚
                """
            else:
                prompt = f"""
                ä»¥ä¸‹æ˜¯ä»Šæ—¥çƒ­ç‚¹ä¿¡æ¯åˆ—è¡¨ï¼ˆåŒ…å«æ–°é—»å’Œç¤¾äº¤åª’ä½“å¸–å­ï¼ŒJSONæ ¼å¼ï¼‰ï¼Œéƒ¨åˆ†æ¡ç›®åŒ…å«å†…å®¹æ‘˜è¦ï¼š
                {hotspot_json}
                è¯·æ€»ç»“å‡º10æ¡æœ€é‡è¦çš„çƒ­ç‚¹æ–°é—»ï¼Œä¼˜å…ˆé€‰æ‹©ç§‘æŠ€å’ŒAIç›¸å…³æ–°é—»ï¼Œä½†ä¹Ÿè¦åŒ…å«å…¶ä»–é¢†åŸŸï¼ˆå¦‚ç¤¾ä¼šã€å¨±ä¹ã€ä½“è‚²ç­‰ï¼‰çš„é‡è¦æ–°é—»ï¼Œå»é™¤é‡å¤å†…å®¹ã€‚
                ä½ éœ€è¦å°†ç›¸ä¼¼çš„æ–°é—»åˆå¹¶ä¸ºä¸€æ¡ï¼Œå¹¶æä¾›ä¸€ä¸ªç›´è§‚ç®€æ´çš„ä¸­æ–‡æ ‡é¢˜ï¼Œéœ€è¦è®²æ¸…æ¥šæ–°é—»å†…å®¹ä¸è¦å¤ªæ³›åŒ–ï¼ˆä¸è¶…è¿‡30ä¸ªå­—ï¼‰ã€‚
                åŒæ—¶ï¼Œä¹Ÿè¯·å…³æ³¨æ¥è‡ª Twitter ç­‰ç¤¾äº¤åª’ä½“æº (source: Twitter) çš„é‡è¦ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯å…³äºæœ€æ–° AI æŠ€æœ¯çªç ´ã€æ¨¡å‹å‘å¸ƒæˆ–é‡è¦è¡Œä¸šåŠ¨æ€çš„å¸–å­ï¼Œå°†å®ƒä»¬ä¸æ–°é—»åŒç­‰å¯¹å¾…è¿›è¡Œç­›é€‰å’Œæ€»ç»“ã€‚
                ç›¸å…³æ–°é—»çš„IDåˆ—è¡¨æœ€å¤šé€‰æ‹©å…¶ä¸­4æ¡ï¼Œå–æœ€å…¸å‹çš„ï¼Œè¶…è¿‡æ•°é‡ä¸éœ€è¦å…¨éƒ¨ç»™å‡ºã€‚è¯·ç‰¹åˆ«æ³¨æ„ï¼Œå¦‚æœåŒä¸€å®¶åª’ä½“åœ¨å¤šä¸ªæ¸ é“å‘å¸ƒç›¸åŒçš„å†…å®¹ï¼Œæˆ–æ–°é—»æ ‡é¢˜ç›¸ä¼¼åº¦æé«˜ï¼Œä¸è¦åŒæ—¶é€‰æ‹©ï¼Œåˆ™ä»…éœ€åˆ—å‡º1æ¡å³å¯ã€‚
                å¦‚æœæœ‰æ‘˜è¦ä¿¡æ¯ï¼Œè¯·å‚è€ƒæ‘˜è¦æä¾›æ›´å‡†ç¡®çš„æ ‡é¢˜ã€‚
                
                è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
                ```json
                [
                  {{
                    "title": "çƒ­ç‚¹æ ‡é¢˜",
                    "related_ids": [ç›¸å…³çƒ­ç‚¹çš„IDåˆ—è¡¨]
                  }},
                  ...
                ]
                ```
                
                åªè¿”å›JSONæ•°æ®ï¼Œä¸è¦æœ‰ä»»ä½•é¢å¤–è¯´æ˜ã€‚
                """
            
            try:
                logger.info(f"æ­£åœ¨è°ƒç”¨ Deepseek APIï¼Œå°è¯•æ¬¡æ•°: {retry_count + 1}/{max_retries}")
                
                payload = {
                    "model": model_id,
                    "messages": [
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»ç¼–è¾‘åŠ©æ‰‹ï¼Œæ“…é•¿å½’çº³æ€»ç»“çƒ­ç‚¹æ–°é—»ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": 0.3,
                    "max_tokens": 1000
                }
                
                import subprocess
                import tempfile
                
                payload_json = json.dumps(payload, ensure_ascii=False)
                
                with tempfile.NamedTemporaryFile(mode='w+', encoding='utf-8', suffix='.json', delete=False) as temp:
                    temp_path = temp.name
                    temp.write(payload_json)
                
                try:
                    curl_cmd = [
                        'curl', '-s', '-X', 'POST',
                        '-H', f'Authorization: Bearer {api_key}',
                        '-H', 'Content-Type: application/json',
                        '-d', f'@{temp_path}',
                        '--insecure',  
                        api_url
                    ]
                    
                    logger.info(f"æ‰§è¡Œcurlå‘½ä»¤: {' '.join(curl_cmd).replace(api_key, '***')}")
                    
                    process = subprocess.run(
                        curl_cmd,
                        check=True,
                        capture_output=True,
                        text=True,
                        encoding='utf-8'
                    )
                    
                    if process.returncode != 0:
                        raise Exception(f"curlå‘½ä»¤æ‰§è¡Œå¤±è´¥ï¼Œè¿”å›ç : {process.returncode}, é”™è¯¯: {process.stderr}")
                    
                    result = json.loads(process.stdout)
                    
                    if "choices" not in result or len(result["choices"]) == 0:
                        raise Exception(f"APIå“åº”æ ¼å¼ä¸æ­£ç¡®: {process.stdout[:200]}...")
                    
                    # Track token usage for Deepseek model
                    if "usage" in result:
                        usage = result["usage"]
                        token_tracker.add_usage(
                            model_id,
                            prompt_tokens=usage.get("prompt_tokens", 0),
                            completion_tokens=usage.get("completion_tokens", 0)
                        )
                    
                    logger.info("APIè°ƒç”¨æˆåŠŸ!")
                    
                finally:
                    try:
                        os.unlink(temp_path)
                    except:
                        pass
                
            except Exception as e:
                logger.error(f"è°ƒç”¨Deepseek APIæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                logger.error(f"é”™è¯¯ç±»å‹: {type(e)}")
                
                import traceback
                logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                
                retry_count += 1
                if retry_count < max_retries:
                    logger.info(f"å°†åœ¨3ç§’åé‡è¯•...")
                    time.sleep(3)
                    continue
                else:
                    raise
            
            json_response = result["choices"][0]["message"]["content"]
            
            json_str = json_response
            if "```json" in json_response:
                json_str = json_response.split("```json")[1].split("```")[0].strip()
            
            output_directory = os.path.join(project_root, "data", "outputs")
            os.makedirs(output_directory, exist_ok=True)
            
            raw_output_filename = os.path.join(output_directory, f"deepseek_raw_response_{today}_{timestamp}.json")
            with open(raw_output_filename, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            logger.info(f"å·²ä¿å­˜DeepseekåŸå§‹å“åº”è‡³ {raw_output_filename}")
            
            output_filename = os.path.join(output_directory, f"deepseek_output_{today}_{timestamp}.json")
            with open(output_filename, 'w', encoding='utf-8') as f:
                f.write(json_str)
            logger.info(f"å·²ä¿å­˜Deepseekè¾“å‡ºæ•°æ®è‡³ {output_filename}")
            
            try:
                news_items = json.loads(json_str)
                
                formatted_summary = ""
                for index, news in enumerate(news_items[:20]):
                    num = str(index + 1).zfill(2)
                    title = news.get("title", "æœªçŸ¥æ ‡é¢˜")
                    
                    formatted_summary += f"## ** {num} {title} **  \n"
                    
                    news_summary = ""
                    related_ids = news.get("related_ids", [])
                    
                    all_summaries = []
                    for news_id in related_ids:
                        if news_id in hotspot_dict:
                            item = hotspot_dict[news_id]
                            if item.get("summary") and len(item.get("summary").strip()) > 20:  
                                all_summaries.append(item.get("summary"))
                    
                    if all_summaries:
                        best_summary = sorted(all_summaries, key=len, reverse=True)[0]
                        if len(best_summary) > 100:
                            news_summary = best_summary[:97] + "..."
                        else:
                            news_summary = best_summary
                    
                    if not news_summary:
                        news_summary = f"{title}ç›¸å…³ä¿¡æ¯ï¼Œè¯¦æƒ…è¯·æŸ¥çœ‹ä»¥ä¸‹é“¾æ¥ã€‚"
                    
                    formatted_summary += f"{news_summary}\n\n"
                    
                    for news_id in related_ids:
                        if news_id in hotspot_dict:
                            item = hotspot_dict[news_id]
                            source_name = SOURCE_NAME_MAP.get(item.get('source', 'unknown'), item.get('source', 'unknown'))
                            item_title = item.get('title', 'æœªçŸ¥æ ‡é¢˜')
                            
                            item_title = item_title.replace('\n', ' ').replace('\r', ' ')
                            item_title = ' '.join(item_title.split())
                            
                            url = item.get('url', '#')
                            if not url or url == "#":
                                if "Twitter" in source_name:
                                    username = source_name.replace("Twitter-", "").strip()
                                    username_no_space = username.replace(" ", "")  # ç§»é™¤ç©ºæ ¼
                                    
                                    # æ¨æ–‡URLæ ¼å¼åº”ä¸º https://twitter.com/username/status/tweet_id
                                    # ç”±äºæˆ‘ä»¬æ²¡æœ‰çœŸå®çš„tweet_idï¼Œæ‰€ä»¥åªèƒ½æŒ‡å‘ç”¨æˆ·ä¸»é¡µ
                                    url = f"https://twitter.com/{username_no_space}"
                                else:
                                    url = "#"  
                            
                            formatted_summary += f"- [{item_title}]({url}) `ğŸ·ï¸{source_name}`\n"
                    
                    formatted_summary += "\n"
                
                summary_filename = os.path.join(output_directory, f"formatted_summary_{today}_{timestamp}.md")
                with open(summary_filename, 'w', encoding='utf-8') as f:
                    f.write(formatted_summary)
                logger.info(f"å·²ä¿å­˜æ ¼å¼åŒ–æ‘˜è¦è‡³ {summary_filename}")
                
                return formatted_summary
                
            except json.JSONDecodeError as e:
                logger.error(f"è§£æDeepseekè¿”å›çš„JSONå¤±è´¥: {str(e)}")
                return f"è§£æDeepseekè¿”å›çš„JSONå¤±è´¥: {str(e)}"
            
        except requests.exceptions.Timeout:
            retry_count += 1
            logger.warning(f"Deepseek API è¯·æ±‚è¶…æ—¶ï¼Œæ­£åœ¨é‡è¯• ({retry_count}/{max_retries})...")
            time.sleep(5)  
        
        except Exception as e:
            logger.error(f"è°ƒç”¨Deepseek APIæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            logger.error(f"é”™è¯¯ç±»å‹: {type(e)}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {e.__dict__ if hasattr(e, '__dict__') else 'No details available'}")
            retry_count += 1
            if retry_count < max_retries:
                logger.warning(f"5ç§’åé‡è¯• ({retry_count}/{max_retries})...")
                time.sleep(5)
            else:
                break
    
    logger.warning("æ— æ³•ä½¿ç”¨Deepseek APIå½’ç±»çƒ­ç‚¹ï¼Œå°†ä½¿ç”¨åŸå§‹çƒ­ç‚¹")
    fallback = ""
    for i, item in enumerate(hotspots[:10]):
        try:
            num = str(i + 1).zfill(2)
            source_name = SOURCE_NAME_MAP.get(item.get('source', 'unknown'), item.get('source', 'unknown'))
            item_title = item.get('title', 'æœªçŸ¥æ ‡é¢˜')
            
            item_title = item_title.replace('\n', ' ').replace('\r', ' ')
            item_title = ' '.join(item_title.split())
            
            fallback += f"## ** {num} {item_title} **  \n"
            
            item_summary = item.get('summary', '')
            if not item_summary:
                item_summary = f"{item_title}ç›¸å…³ä¿¡æ¯ï¼Œè¯¦æƒ…è¯·æŸ¥çœ‹ä»¥ä¸‹é“¾æ¥ã€‚"
            elif len(item_summary) > 100:
                item_summary = item_summary[:97] + "..."
            
            fallback += f"{item_summary}\n\n"
            
            url = item.get('url', '#')
            if not url or url == "#":
                if "Twitter" in source_name:
                    username = source_name.replace("Twitter-", "").strip()
                    username_no_space = username.replace(" ", "")  
                    url = f"https://twitter.com/{username_no_space}"
            
            fallback += f"- [{item_title}]({url}) `ğŸ·ï¸{source_name}` \n\n"
        except Exception as e:
            logger.error(f"å¤„ç†å¤‡é€‰çƒ­ç‚¹æ—¶å‡ºé”™(è·³è¿‡æ­¤æ¡): {str(e)}")
            continue
    
    return fallback